{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMARDcast: Day-Ahead Forecasting of German Electricity Consumption with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nick Krüger, Kolja Eger, Wolfgang Renz\n",
    "\n",
    "E-Mail: {nick.krueger, kolja.eger, wolfgang.renz}@haw-hamburg.de\n",
    "\n",
    "### Published in: International Conference on Smart Energy System and Technologies (SEST) 2024\n",
    "\n",
    "URL: https://ieeexplore.ieee.org/document/10694018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a practical example of the work published in the publication \"SMARDcast: Day-Ahead Forecasting of German Electricity Consumption with Deep Learning\". The aim of this work was to showcase the application of deep neural networks for day-ahead forecasting of electricity consumption in Germany, using the power consumption data sets published on the SMARD plattform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "The following libraries are used to run this notebook. The repository also includes a requirements file of all used libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') \n",
    "for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from deutschland import feiertage\n",
    "from deutschland.feiertage.api import default_api\n",
    "configuration = feiertage.Configuration(\n",
    "    host = \"https://feiertage-api.de/api\"\n",
    ")\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import TwoSlopeNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "For this experiment, we use the German electricity market data published by the German regulator on the SMARD platform. The individual .csv files contain a year of German power consumption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SMARD = pd.DataFrame()\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/koljaeger/smardcast/main/data/Realisierter_Stromverbrauch_\"\n",
    "\n",
    "for year in range(2015, 2023+1, 1):\n",
    "    df_SMARD = pd.concat([df_SMARD, pd.read_csv(url+str(year)+\"01010000_\"+str(year)+\"12312359_Viertelstunde.csv\", \n",
    "                                    delimiter=\";\", thousands='.', decimal=\",\", dtype={\"Datum\":str})], axis=0, ignore_index=True)\n",
    "\n",
    "df_SMARD[\"Date\"] = pd.to_datetime(df_SMARD.pop(\"Datum\")+' '+df_SMARD.pop(\"Anfang\"), format=\"%d.%m.%Y %H:%M\")\n",
    "df_SMARD[\"Date\"] = df_SMARD[\"Date\"].dt.tz_localize(\"Europe/Berlin\", ambiguous='infer').dt.tz_convert('UTC')\n",
    "\n",
    "df_SMARD = df_SMARD.rename(\n",
    "    columns={\n",
    "        'Gesamt (Netzlast) [MWh] Originalauflösungen': 'Total Load [MWh]',\n",
    "        'Residuallast [MWh] Originalauflösungen': 'Residual Load [MWh]',\n",
    "        'Pumpspeicher [MWh] Originalauflösungen' : 'Energy from Pumped Storage [MWh]'\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_SMARD = df_SMARD.drop(columns=[\"Ende\",'Residual Load [MWh]', 'Energy from Pumped Storage [MWh]'])\n",
    "\n",
    "print(df_SMARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Time Features\n",
    "\n",
    "To aid the model in understanding the seasonality of the SMARD power consumption dataset, we add additional input features. These use the timestamps of the dataset to generate sin and cosine values with the period of 1 hour, 1 day, 1 week, 1 month and 1 year for each timestep in the data set. We also add a categorical feature indicating the day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    new_columns = [(column, \"\") for column in df.columns]\n",
    "      \n",
    "    new_df = df\n",
    "    new_df.columns=pd.MultiIndex.from_tuples(new_columns)\n",
    "\n",
    "    times_dict = {\"Hour Func\" : (60*60), \"Day Func\" : (24*60*60), \"Week Func\" : (7*24*60*60), \"Month Func\" : (365.2425*24*60*60/12), \"Year Func\" : (365.2425*24*60*60)}\n",
    "\n",
    "    num_of_seconds = (df[\"Date\"].dt.tz_localize(None) - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "\n",
    "    for column in times_dict:\n",
    "        time_func_df = pd.DataFrame({\"Sin\" : np.sin(num_of_seconds * (2 * np.pi / times_dict[column])), \"Cos\" : np.cos(num_of_seconds * (2 * np.pi / times_dict[column]))})\n",
    "        time_func_columns = [(column,\"Sin\"), (column,\"Cos\")]\n",
    "        time_func_df.columns = pd.MultiIndex.from_tuples(time_func_columns)\n",
    "    \n",
    "        new_df = pd.concat([new_df,time_func_df], axis=1)\n",
    "\n",
    "    new_df[\"Day of the Week\"] = df[\"Date\"].dt.dayofweek\n",
    "\n",
    "    print(\"Time Step Sine Functions and Categories Added!\")\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SMARD = add_time_features(df_SMARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Holiday Features\n",
    "\n",
    "In addition, we also add an additional feature to model public holidays. For this purpose, we use an api call to get the german public holidays for each year of the dataset. Public holidays in the data set are modeled as 1, days before public holidays as 0.5 and other days as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_holiday_features(df):\n",
    "    lowest_year = df[\"Date\"].dt.year.min()\n",
    "    highest_year = df[\"Date\"].dt.year.max()\n",
    "\n",
    "    holiday_dates = []\n",
    "\n",
    "    with feiertage.ApiClient(configuration) as api_client:\n",
    "        api_instance = default_api.DefaultApi(api_client)\n",
    "\n",
    "    for jahr in range(lowest_year, highest_year+1):\n",
    "        try:\n",
    "            api_response = api_instance.get_feiertage(jahr=str(jahr), nur_land=\"HH\", nur_daten=1)\n",
    "        except feiertage.ApiException as e:\n",
    "            print(\"Exception when calling DefaultApi->get_feiertage: %s\\n\" % e)\n",
    "\n",
    "        for feiertag in api_response:\n",
    "            holiday_dates.append(api_response[feiertag])\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format='%Y-%m-%d %H:%M:%S')\n",
    "    df_days = df[\"Date\"].dt.date\n",
    "\n",
    "    days_before_holidays = {date - datetime.timedelta(days=1) for date in holiday_dates}\n",
    "\n",
    "    df['Is Holiday'] = df_days.apply(lambda x: 1 if x in holiday_dates else (0.5 if x in days_before_holidays else 0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SMARD = add_holiday_features(df_SMARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data Set\n",
    "\n",
    "The data set is then split into training, validation and testing data sets. In this example, we use the years 2015 - 2020 as training data, the years 2021 - 2022 as validation data and 2023 as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df, train_start_date, train_stop_date, val_start_date, val_stop_date, test_start_date, test_stop_date):\n",
    "    train_df = df[(df[\"Date\"] >= train_start_date)]\n",
    "    train_df = train_df[(train_df[\"Date\"] <= train_stop_date)]\n",
    "\n",
    "    val_df = df[(df[\"Date\"] >= val_start_date)]\n",
    "    val_df = val_df[(val_df[\"Date\"] <= val_stop_date)]\n",
    "\n",
    "    test_df = df[(df[\"Date\"] >= test_start_date)]\n",
    "    test_df = test_df[(test_df[\"Date\"] <= test_stop_date)]\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_df(df_SMARD, train_start_date=\"2015-01-01 00:00\", train_stop_date=\"2020-12-31 23:45\",\n",
    "                                     val_start_date=\"2021-01-01 00:00\", val_stop_date=\"2022-12-31 23:45\", \n",
    "                                     test_start_date=\"2023-01-01 00:00\", test_stop_date=\"2023-12-31 23:45\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization\n",
    "\n",
    "Before the data sets are used for training, they are standardized using mean-std-standardization. The mean and std values are calculated using only the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(train_df, val_df, test_df):\n",
    "\n",
    "    train_date = train_df.pop(\"Date\")\n",
    "        \n",
    "    mean = train_df.mean()\n",
    "    std = train_df.std()\n",
    "\n",
    "    std_train_df = (train_df - mean) / std\n",
    "    std_train_df[\"Date\"] = train_date\n",
    "\n",
    "    val_date = val_df.pop(\"Date\")\n",
    "    std_val_df = (val_df - mean) / std\n",
    "    std_val_df[\"Date\"] = val_date\n",
    "\n",
    "    test_date = test_df.pop(\"Date\")\n",
    "    std_test_df = (test_df - mean) / std\n",
    "    std_test_df[\"Date\"] = test_date\n",
    "\n",
    "    print(\"Mean of training dataset:\\n\" + str(mean) + \"\\n\")\n",
    "    print(\"Standard deviation of training dataset:\\n\" + str(std))\n",
    "    \n",
    "    return std_train_df, std_val_df, std_test_df, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_standardize_data(df, columns, mean, std):\n",
    "    return df*float(std.loc[columns].iloc[0]) + float(mean.loc[columns].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, mean, std = standardize_data(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Windowing\n",
    "\n",
    "In order to create the indiviual input and output sets for each training, validation and testing step, the data sets are windowed. In this case, the last input of each input window is at 9:45 am the day before the forecasted day, while the output window is an entire day. The input window in this example contains 672 timesteps, which contains one week worth of timesteps. These windows are created for every day of the training, validaytion and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_df(df, input_columns, label_columns, input_timesteps, label_timesteps, offset, window_split):\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "        \n",
    "    timestep_index = []\n",
    "    for i in range(0, len(df), window_split):\n",
    "        if i > (input_timesteps + offset) and (len(df) - i) >= (label_timesteps):\n",
    "            timestep_index.append(i)\n",
    "    \n",
    "    input_df = df[input_columns]\n",
    "    label_df = df[label_columns]\n",
    "\n",
    "    inputs = np.empty((len(timestep_index), input_timesteps, len(input_df.columns)), dtype=\"float32\")\n",
    "    labels = np.empty((len(timestep_index), label_timesteps, len(label_df.columns)), dtype=\"float32\")\n",
    "\n",
    "    input_date = []\n",
    "    label_date = []\n",
    "\n",
    "    for (new_window, window_index) in zip(timestep_index, range(len(timestep_index))):\n",
    "        inputs[window_index,:,:] = input_df.loc[(new_window - offset - input_timesteps) : (new_window - offset -1)].to_numpy()\n",
    "        labels[window_index,:,:] = label_df.loc[(new_window) : (new_window + label_timesteps-1)].to_numpy()\n",
    "\n",
    "        input_date.append(df[\"Date\"].loc[(new_window - offset - input_timesteps) : (new_window - offset -1)].tolist())\n",
    "        label_date.append(df[\"Date\"].loc[(new_window) : (new_window + label_timesteps-1)].tolist())\n",
    "\n",
    "    print(\"Input Shape: \" + str(inputs.shape))\n",
    "    print(\"Output Shape: \" + str(labels.shape))\n",
    "\n",
    "    return inputs, labels, input_date, label_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = [\"Total Load [MWh]\", \"Hour Func\", \"Week Func\", \"Month Func\", \"Year Func\", \"Day of the Week\", \"Is Holiday\"]\n",
    "output_columns = [\"Total Load [MWh]\"]\n",
    "\n",
    "input_timesteps = 672\n",
    "output_timesteps = 96\n",
    "\n",
    "offset = 56\n",
    "window_split=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_labels, train_dates_2, train_dates = window_df(df=train_df, input_columns=input_columns, input_timesteps=input_timesteps,\n",
    "                                            label_columns=output_columns, label_timesteps=output_timesteps, \n",
    "                                            offset=offset, window_split=window_split)\n",
    "\n",
    "val_input, val_labels, _, val_labels_dates = window_df(df=val_df, input_columns=input_columns, input_timesteps=input_timesteps,\n",
    "                                            label_columns=output_columns, label_timesteps=output_timesteps, \n",
    "                                            offset=offset, window_split=window_split)\n",
    "\n",
    "test_input, test_labels, test, test_labels_dates = window_df(df=test_df, input_columns=input_columns, input_timesteps=input_timesteps,\n",
    "                                            label_columns=output_columns, label_timesteps=output_timesteps, \n",
    "                                            offset=offset, window_split=window_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CNN-LSTM Model\n",
    "\n",
    "Using tensorflow and Keras, we create and train a CNN-LSTM model with the hyperparamters as specifed in the publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.Input(shape=[train_input.shape[1], train_input.shape[2]])\n",
    "\n",
    "hidden = keras.layers.Conv1D(256, kernel_size=4)(input)\n",
    "hidden = keras.layers.BatchNormalization()(hidden)\n",
    "hidden = keras.layers.MaxPooling1D()(hidden)\n",
    "\n",
    "hidden = keras.layers.LSTM(384)(hidden)\n",
    "\n",
    "output = keras.layers.Dense(train_labels.shape[1], activation='linear', kernel_initializer=tf.initializers.zeros(), dtype='float32')(hidden)\n",
    "\n",
    "model = keras.models.Model(input, output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.fit(train_input, train_labels, epochs=50, batch_size=8, callbacks=[stop_early], validation_data=[val_input, val_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Evaluation\n",
    "\n",
    "After training, the network is evaulated by predicting the validation and testing data set inputs and comparing the forecasts by the network to the true values. The results are saved in pandas Dataframes, with each value being a single forecast timestep. We also calculate the MAE and MAPE for each foreacst, which allows us to compare the performance of the network to the forecast from the SMARD platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_forecast = model.predict(val_input)\n",
    "test_forecast = model.predict(test_input)\n",
    "\n",
    "val_forecast = inv_standardize_data(val_forecast, columns=output_columns, mean=mean, std=std)\n",
    "test_forecast = inv_standardize_data(test_forecast, columns=output_columns, mean=mean, std=std)\n",
    "\n",
    "val_true_values = inv_standardize_data(val_labels, columns=output_columns, mean=mean, std=std).reshape(val_labels.shape[:-1])\n",
    "test_true_values = inv_standardize_data(test_labels, columns=output_columns, mean=mean, std=std).reshape(test_labels.shape[:-1])\n",
    "\n",
    "val_results = pd.DataFrame({\n",
    "    \"Date\" : list(itertools.chain.from_iterable(val_labels_dates)),\n",
    "    \"True Value\" : list(itertools.chain.from_iterable(val_true_values)),\n",
    "    \"Model Forecast\" : list(itertools.chain.from_iterable(val_forecast)),\n",
    "    })\n",
    "\n",
    "test_results = pd.DataFrame({\n",
    "    \"Date\" : list(itertools.chain.from_iterable(test_labels_dates)),\n",
    "    \"True Value\" : list(itertools.chain.from_iterable(test_true_values)),\n",
    "    \"Model Forecast\" : list(itertools.chain.from_iterable(test_forecast)),\n",
    "    })\n",
    "\n",
    "val_results[\"Model Absolute Error\"] = abs(val_results[\"True Value\"] - val_results[\"Model Forecast\"])\n",
    "test_results[\"Model Absolute Error\"] = abs(test_results[\"True Value\"] - test_results[\"Model Forecast\"])\n",
    "\n",
    "val_results[\"Model Absolute Percentage Error\"] = (abs(val_results[\"True Value\"] - val_results[\"Model Forecast\"])/val_results[\"True Value\"]*100)\n",
    "test_results[\"Model Absolute Percentage Error\"] = (abs(test_results[\"True Value\"] - test_results[\"Model Forecast\"])/test_results[\"True Value\"]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SMARD Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_SMARD_prediction_data(path, remove_bad_columns=False):\n",
    "\n",
    "    df = pd.read_csv(path, delimiter=';', thousands='.', decimal=',', dtype={\"Datum\":str})\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df.pop(\"Datum\")+' '+df.pop(\"Anfang\"), format=\"%d.%m.%Y %H:%M\")\n",
    "    df[\"Date\"] = df[\"Date\"].dt.tz_localize(\"Europe/Berlin\", ambiguous='infer').dt.tz_convert('UTC')\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "        'Gesamt (Netzlast) [MWh] Originalauflösungen': 'Total Load [MWh]',\n",
    "        'Residuallast [MWh] Originalauflösungen': 'Residual Load [MWh]'\n",
    "        }\n",
    "    )\n",
    "    if remove_bad_columns==True:\n",
    "        df = df.drop(['Residual Load [MWh]'], axis=\"columns\")\n",
    "        df.pop('Ende')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/koljaeger/smardcast/main/data/Prognostizierter_Stromverbrauch_\"\n",
    "\n",
    "SMARD_prediction_df = pd.concat([read_SMARD_prediction_data(url+\"202101010000_202112312359_Viertelstunde.csv\", remove_bad_columns=True),\n",
    "                                read_SMARD_prediction_data(url+\"202201010000_202212312359_Viertelstunde.csv\", remove_bad_columns=True),\n",
    "                                read_SMARD_prediction_data(url+\"202301010000_202312312359_Viertelstunde.csv\", remove_bad_columns=True)])\n",
    "\n",
    "val_results[\"SMARD Forecast\"] = list(SMARD_prediction_df[\"Total Load [MWh]\"][(SMARD_prediction_df[\"Date\"] >= val_results[\"Date\"].iloc[0]) & (val_results[\"Date\"].iloc[-1] >= SMARD_prediction_df[\"Date\"])])\n",
    "test_results[\"SMARD Forecast\"] = list(SMARD_prediction_df[\"Total Load [MWh]\"][(SMARD_prediction_df[\"Date\"] >= test_results[\"Date\"].iloc[0]) & (test_results[\"Date\"].iloc[-1] >= SMARD_prediction_df[\"Date\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results[\"SMARD Absolute Error\"] = abs(val_results[\"True Value\"] - val_results[\"SMARD Forecast\"])\n",
    "test_results[\"SMARD Absolute Error\"] = abs(test_results[\"True Value\"] - test_results[\"SMARD Forecast\"])\n",
    "\n",
    "val_results[\"SMARD Absolute Percentage Error\"] = (abs(val_results[\"True Value\"] - val_results[\"SMARD Forecast\"])/val_results[\"True Value\"]*100)\n",
    "test_results[\"SMARD Absolute Percentage Error\"] = (abs(test_results[\"True Value\"] - test_results[\"SMARD Forecast\"])/test_results[\"True Value\"]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([val_results, test_results])\n",
    "results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization\n",
    "\n",
    "After the evaulation, the results are visualized in multiple plots, similar to the plots in the publication. We compare the forecasting performance using barplots, calculating the mean performance across days of the week and months, as well as showcasing the forecasts during public holidays. We also create boxplots showcasing the difference in forecasting performance across days of the week and time of days, as well as days and months.\n",
    "\n",
    "### Barplot Days of the Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Weekday\"] = results['Date'].dt.day_name().str.slice(0, 3)\n",
    "order_days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "results['Weekday'] = pd.Categorical(results['Weekday'], categories=order_days, ordered=True)\n",
    "\n",
    "weekday_mean_model_APE = results.groupby(['Weekday'], observed=True)[\"Model Absolute Percentage Error\"].mean()\n",
    "weekday_mean_SMARD_APE = results.groupby(['Weekday'], observed=True)[\"SMARD Absolute Percentage Error\"].mean()\n",
    "\n",
    "mape_data = pd.DataFrame({\n",
    "    'Weekday': order_days,\n",
    "    'CNN-LSTM': weekday_mean_model_APE,\n",
    "    'SMARD Forecast': weekday_mean_SMARD_APE\n",
    "})\n",
    "\n",
    "\n",
    "mape_data.plot(kind='bar', figsize=(20, 8), color=[\"orange\",\"red\",\"blue\"])\n",
    "\n",
    "plt.xlabel('Day of the Week', fontsize = 15)\n",
    "plt.ylabel('Mean Absolute Percentage Error (MAPE) [%]', fontsize = 15)\n",
    "plt.xticks(rotation=0, fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Year Month'] = results['Date'].dt.strftime('%m %Y')\n",
    "\n",
    "order_year_month = [\"01 2021\", \"02 2021\", \"03 2021\", \"04 2021\", \"05 2021\", \"06 2021\", \"07 2021\", \"08 2021\", \"09 2021\", \"10 2021\", \"11 2021\", \"12 2021\",\n",
    "                        \"01 2022\", \"02 2022\", \"03 2022\", \"04 2022\", \"05 2022\", \"06 2022\", \"07 2022\", \"08 2022\", \"09 2022\", \"10 2022\", \"11 2022\", \"12 2022\",\n",
    "                        \"01 2023\", \"02 2023\", \"03 2023\", \"04 2023\", \"05 2023\", \"06 2023\", \"07 2023\", \"08 2023\", \"09 2023\", \"10 2023\", \"11 2023\", \"12 2023\",]\n",
    "\n",
    "results[\"Year Month\"] = pd.Categorical(results[\"Year Month\"], categories=order_year_month, ordered=True)\n",
    "\n",
    "monthly_mean_model_APE = results.groupby(['Year Month'], observed=True)[\"Model Absolute Percentage Error\"].mean()\n",
    "monthly_mean_SMARD_APE = results.groupby(['Year Month'], observed=True)[\"SMARD Absolute Percentage Error\"].mean()\n",
    "\n",
    "mape_data = pd.DataFrame({\n",
    "    'Year Month': order_year_month,\n",
    "    'CNN-LSTM': monthly_mean_model_APE,\n",
    "    'SMARD Forecast': monthly_mean_SMARD_APE\n",
    "})\n",
    "\n",
    "mape_data.plot(kind='bar', figsize=(20, 8), color=[\"orange\",\"red\",\"blue\"])\n",
    "\n",
    "plt.xlabel('Month', fontsize = 15)\n",
    "plt.ylabel('Mean Absolute Percentage Error (MAPE) [%]', fontsize = 15)\n",
    "plt.xticks(range(0, len(mape_data), 3), rotation=45, fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Example during Public Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(20, 8))\n",
    "\n",
    "results['Year Month Day'] = results['Date'].dt.strftime('%Y %m %d')\n",
    "\n",
    "week_data = results[results['Year Month Day'].between('2022 10 02', '2022 10 08')]\n",
    "\n",
    "plt.plot(week_data['Date'], week_data['True Value'], color='black', label='True Value')\n",
    "plt.plot(week_data['Date'], week_data['Model Forecast'], color='orange', label='CNN-LSTM')\n",
    "plt.plot(week_data['Date'], week_data['SMARD Forecast'], color='red', label='SMARD')\n",
    "plt.xlabel('Datum', fontsize = 15)\n",
    "plt.ylabel('Gesamt (Netzlast) [MWh]', fontsize = 15)\n",
    "plt.ylim(9000, 19000)\n",
    "plt.tick_params(axis='both', labelsize = 15)\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d.%m.%y'))\n",
    "plt.legend(fontsize = 15, loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Day of Week - Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results[\"Comp\"] = results[\"SMARD Absolute Percentage Error\"] - results[\"Model Absolute Percentage Error\"]\n",
    "\n",
    "results[\"Weekday\"] = results['Date'].dt.day_name().str.slice(0, 3)\n",
    "order_days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "results['Weekday'] = pd.Categorical(results['Weekday'], categories=order_days, ordered=True)\n",
    "\n",
    "results['Time'] = results['Date'].dt.strftime(\"%H:%M\")\n",
    "\n",
    "heatmap_data = results.pivot_table(index='Weekday', columns='Time', values=\"Comp\", aggfunc=\"mean\")\n",
    "    \n",
    "divnorm = TwoSlopeNorm(vmin=-4, vcenter=0, vmax=4)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.heatmap(heatmap_data, cmap=\"vlag\", norm=divnorm, annot=False, linewidths=.2, cbar=False, xticklabels=6)\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "ax.set_xlabel('Time', fontsize = 15)\n",
    "ax.set_ylabel('Weekday', fontsize = 15)\n",
    "\n",
    "cbar = plt.colorbar(ax.collections[0])   \n",
    "cbar.outline.set_visible(False)\n",
    "cbar.set_label('MAPE SMARD Forecast - MAPE NN Forecast [%]', fontsize=15)\n",
    "cbar.set_ticks([-4, -2, 0, 2, 4])\n",
    "cbar.set_ticklabels(['-4', '-2', '0', '2', '4'], fontsize=15)\n",
    "    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Months - Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Day'] = results['Date'].dt.strftime('%d')\n",
    "\n",
    "heatmap_data = results.pivot_table(index='Year Month', columns='Day', values=\"Comp\", aggfunc=\"mean\")\n",
    "divnorm = TwoSlopeNorm(vmin=-20, vcenter=0, vmax=20)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.heatmap(heatmap_data, cmap=\"vlag\", norm=divnorm, annot=False, linewidths=.2, cbar=False, xticklabels=3, yticklabels=3)\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "ax.set_xlabel('Day', fontsize = 15)\n",
    "ax.set_ylabel('Month', fontsize = 15)\n",
    "\n",
    "cbar = plt.colorbar(ax.collections[0])\n",
    "cbar.outline.set_visible(False)\n",
    "cbar.set_label('MAPE SMARD Forecast - MAPE NN Forecast [%]', fontsize=15)\n",
    "cbar.set_ticks([-20, -15, -10, -5, 0, 5, 10, 15, 20])\n",
    "cbar.set_ticklabels(['-20', '-15', '-10', '-5', '0', '5', '10', '15', '20'], fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
